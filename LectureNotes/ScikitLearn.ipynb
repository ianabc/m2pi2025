{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d36c09-0a2d-4bbb-b6d0-1bc8ee1e81ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (16,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a42bebf-da26-4f2f-b394-edfb4f4c1956",
   "metadata": {},
   "source": [
    "# scikit-learn\n",
    "\n",
    "Scikit-Learn is one of the most widely used python modules for machine learning. It has implementations of a huge number of algorithms and helper tools and (as far as possible) provides a consistent and easy to use interface to them. \n",
    "\n",
    "_**Caveat**: Machine learning is a _huge_ topic and I'm not an expert in it. These notes cover some introductory steps and useful tips, but you should augment them with other resources as you learn more. The [scikit-learn tutorial](https://scikit-learn.org/stable/tutorial/index.html) is a good place to start, and there are lots of [courses](https://medium.com/javarevisited/5-best-scikit-learn-online-courses-for-machine-learning-and-data-science-6beb02e9cca0) and [books](https://www.amazon.ca/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646/ref=sr_1_1?keywords=hands-on+machine+learning+with+scikit-learn%2C+keras%2C+and+tensorflow&qid=1657479285&sprefix=scikit-learn%2Caps%2C425&sr=8-1) available._\n",
    "\n",
    "## The Problem\n",
    "\n",
    "Machine Learning problems usually start with a (potentially large) collection of data which you want to use to draw inferences from to make predictions. There are two broad categories of machine learning algorithms:\n",
    "\n",
    "  * **Supervised Learning**: In supervised learning, the data you are learning from comes with the answers: it is _labeled_. The idea is to use this labeling to train your algorithm to make predictions for new data which lack the labels. There are two broad categories of problem where supervised learning is used:\n",
    "  \n",
    "      * **Classification Problems**: where you want to use the data to divide instances into discrete categories (e.g. Logistic Regression)\n",
    "      * **Regression Problems**: where you want to use the data to make some numerical predictions (e.g. Linear Regression)\n",
    "      \n",
    "      \n",
    "  * **Unsupervised Learning**: In unsupervised learning the learning algorithm has to learn for itself. It has no labels to guide it, only the data. Some of the most common categories or problem tackled with unsupervised learning algorithms are:\n",
    "      * **Clustering**: Recognizing distinct groups within the data (e.g. K-Means)\n",
    "      * **Dimensionality Reduction** Making datasets more tractable while retaining correlations and patterns (e.g. Principal Component Analysis (PCA)\n",
    "      * **Anomoly/Novelty Detection** Finding outliers in your dataset (e.g. One class Support Vector Machine)\n",
    "\n",
    "The landscape of machine learning algorithms is huge and continues to grow, we don't have any hope of covering all of them, but if you can categorize your problem similar to those above, some searching and comparing to existing problems will normally help you narrow down the approaches you want to try. Fortunately scikit-learn provides a consistent interface to many of them and makes it possible to explore and experiment until you find the one(s) you need! The Scikit-Learn documentation has a nice [Choosing the right estimator](https://scikit-learn.org/stable/machine_learning_map.html) page.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0a7c70-a366-443b-a756-8aa8727d7f8c",
   "metadata": {},
   "source": [
    "## A (very basic) scikit-learn recipe\n",
    "\n",
    "1. Ingest the data\n",
    "1. Split the data into training and test sets\n",
    "1. Prepare the data, tidy missing values, scale etc.\n",
    "1. Train your model\n",
    "1. Make predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782f225d-604c-4aa3-9047-f0a1a5fa8e1e",
   "metadata": {},
   "source": [
    "We'll work through an example from Aurélien Géron's book [Hands on Machine Learning with scikit-learn](https://www.amazon.ca/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646/ref=sr_1_1?keywords=hands-on+machine+learning+with+scikit-learn%2C+keras%2C+and+tensorflow&qid=1657479285&sprefix=scikit-learn%2Caps%2C425&sr=8-1). **The content below is a repetition of Aurélien's analysis (which is also available as a [notebook](https://github.com/ageron/handson-ml2/blob/master/02_end_to_end_machine_learning_project.ipynb)) with a bit of commentary.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e86b7-1a0d-4c12-a6d0-671d8e2e2fd6",
   "metadata": {},
   "source": [
    "Starting at step 1, you're experience with `numpy` and `pandas` should help you a lot. We'll assume that the problem set is small enough that it fits in memory on a single machine (e.g. a reasonably small csv) so you can read it in with `pd.read_csv`. For some problems this won't be true and you need to look at algorithms which can work with new data \"on-the-fly\", these are typically referred to as _online learning_.\n",
    "\n",
    "The dataset we will use looks at house prices in California and the goal is to try to predict the prices of houses based on other criteria such as the age of the house, the income of the occupents, the number of rooms, etc. In the terminology above, this is a supervised learning regression problem. The next thing to do would be to take a quick look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74c1725-b4f9-464e-9dae-cdb7ed8fd6f7",
   "metadata": {},
   "source": [
    "## 1. Ingest the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2876f012-d316-4e00-8e7f-9ff395884c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path, filter='tar')\n",
    "    housing_tgz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aa3409-9b90-4fed-b699-c7a4eadc109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4604e2b8-e84f-49ac-90e7-d736a61f0292",
   "metadata": {},
   "outputs": [],
   "source": [
    "housesDF = pd.read_csv(os.path.join(HOUSING_PATH, \"housing.csv\"))\n",
    "housesDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b32cf0-fb74-4538-b318-792561310993",
   "metadata": {},
   "outputs": [],
   "source": [
    "housesDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3591e6-eb96-427a-8559-fe79038756e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "housesDF.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903e41e9-600f-4606-aeac-624e43061197",
   "metadata": {},
   "outputs": [],
   "source": [
    "housesDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63f1bf-e5ef-4acc-b93a-26d2beff2d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "housesDF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a532cb1-de63-401b-a038-06d0dbfc34ee",
   "metadata": {},
   "source": [
    "`ocean_proximity` is a categorical variable..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a961a836-43e8-4b14-ab38-b1d5d941949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "housesDF['ocean_proximity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58084be2-7713-4531-b700-de7245918627",
   "metadata": {},
   "source": [
    "For the numerical values histograms can be useful to orient yourself with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d46cde-bab6-4faf-87c5-9fdd9d682bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "housesDF.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee44a48b-c6c6-437c-9e8d-08c93423f81b",
   "metadata": {},
   "source": [
    "There are a few things to notice here\n",
    "\n",
    "1. The values follow various distribution(s)\n",
    "2. They cover a variety of scales\n",
    "3. Some values (e.g. `median_house_value` and `housing_median_age` have extreme values which probably represent some sort of cut off or binning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f0ed35-eeae-4c12-8fef-5c8c68df63f6",
   "metadata": {},
   "source": [
    "## Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5423f18f-8c45-4472-b6b6-3b886c48c098",
   "metadata": {},
   "source": [
    "It is important to do this early. The risk is that you'll bias your models and end up with bad generalization errors otherwise. Given that we're going to be playing with feature engineering, scaling and other transformations, this might seem premature, but we'll use Pipelines to address that. At this point, it would also be good to start thinking about selecting a performance measure. For a regression like this RMSE is simple and will probably be sufficient, but the documentation has a section on [metrics and scoring](https://scikit-learn.org/stable/modules/model_evaluation.html) which can help you decide more complicated cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd20b7-7599-4688-a60c-a5091637c926",
   "metadata": {},
   "source": [
    "When splitting the training and test set, we want to make sure that both sets are sampled without bias. In our example, we are to assume that `median_income` will be important to our final housing price predictions and we want to make sure that our training set includes data from the full range of incomes. Since `median_income` is an (almost) continuous variable, the idea is to bin the values into categories, then use those categories to sample from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a17eac-694d-4e63-8d3f-01a00e78a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "housesDF['income_cat'] = pd.cut(housesDF['median_income'],\n",
    "       bins = [0, 1.5, 3.0, 4.5, 6.0, np.inf],\n",
    "       labels = [1, 2, 3, 4, 5]\n",
    ")\n",
    "housesDF['income_cat'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e7d6b0-22aa-4f3a-87e0-3fa40be88972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits = 1, test_size=0.2, random_state=42)\n",
    "for train_idx, test_idx in split.split(housesDF, housesDF['income_cat']):\n",
    "    trainDForig = housesDF.loc[train_idx]\n",
    "    testDF = housesDF.loc[test_idx]\n",
    "\n",
    "trainDF = trainDForig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed84facf-dba3-4f77-a913-14a1ae92a889",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['income_cat'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecca288-9155-4ef6-98dc-9a7d9fc9149b",
   "metadata": {},
   "source": [
    "So our training set looks like it samples the various income categories well. We no longer need the `income_cat` column, so we'll drop it from both sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef876664-77cb-41d3-b0ff-00ecef1de28f",
   "metadata": {},
   "source": [
    "## 3. Prepare the data, tidy missing values etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59913221-cdd4-4fe3-93f2-976797247fa8",
   "metadata": {},
   "source": [
    "The histograms for latitude and longitude are probably hiding some structure, all of the values should fall somewhere on a map of California..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b5ef5b-26c2-4979-a107-fbf02bb891f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df2243e-4095-4470-83ad-b6fb5ca07948",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.plot(\n",
    "    kind='scatter',\n",
    "    x='longitude',\n",
    "    y='latitude',\n",
    "    alpha=0.4,\n",
    "    s=trainDF['population']/100,\n",
    "    label='population',\n",
    "    c='median_house_value',\n",
    "    cmap=plt.get_cmap(\"jet\"),\n",
    "    colorbar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35ae8f3-472e-498c-8242-e7186bf35aec",
   "metadata": {},
   "source": [
    "After taking a look at the individual features, the next thing is to look at combinations of features. Often this can help you prepare the dataset so that the algorithms you select have the best chance for success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83330bd3-0b18-411e-81df-f9eec845b6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26286ae6-520c-48e3-bc21-5c9182c45ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.matshow(trainDF.corr(numeric_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a672f965-5383-46f0-89b5-ef55b2fc46fe",
   "metadata": {},
   "source": [
    "To dig a little deeper, you can look at the `scatter_matrix` which plots the numerical values of features against each other "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68136db7-3121-4fcb-9ef8-9277d81d714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\n",
    "    'median_house_value',\n",
    "    'median_income',\n",
    "    'total_rooms',\n",
    "    'housing_median_age'\n",
    "]\n",
    "scatter_matrix(trainDF[attributes])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa3eb5a-0bd5-4efd-9b03-166f7ee3299e",
   "metadata": {},
   "source": [
    "The `median_income` shows strong positive correlation with the `median_house_value` (see the [wikipedia entry for correlation](https://en.wikipedia.org/wiki/Correlation)). You can see horizontal lines showing a hard cap of \\\\$500,000 as well as fainter lines clustered around some other figures (\\\\$450,000, \\\\$350,000) etc. These are likely to be artifacts of the way the data was collected or encoded and you should decide what to do with them. For example, it might be worth trying to train your model without the entries valued at \\\\$500,000 (assuming that the actual values were larger but were encoded as \\\\$500,000 during data entry)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d29c26-38be-4fcf-94a3-f3a7727a6d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.drop(trainDF[trainDF['median_house_value']>=500000].index, inplace=True)\n",
    "trainDF.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74737355-f1ce-4d40-a930-50c87fee60a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots()\n",
    "ax.scatter(trainDF['median_income'], trainDF['median_house_value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1e887c-ab9f-4a9c-8584-3f7ba0140de9",
   "metadata": {},
   "source": [
    "It is also useful to look at combinations at attributes. For example, we have the total number of households per district and the total number of bedrooms, but we could combine those to give the number of rooms per household..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bd6747-35dd-47d4-a6bb-1d7843f4b33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['rooms_per_household'] = trainDF['total_rooms'] / trainDF['households']\n",
    "trainDF['bedrooms_per_room'] = trainDF['total_bedrooms'] / trainDF['total_rooms']\n",
    "trainDF['population_per_household'] = trainDF['population'] / trainDF['households']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822a26d2-bde2-47f9-9261-03c274cd3c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58cd981-0f80-447f-901a-d60a49cd09c8",
   "metadata": {},
   "source": [
    "To deal with missing values, you can either discard the corresponding data, or replace them with some useful value. Personally, I usually try discarding them first, but if that isn't feasible (throwing away too much valuable data), then scikit-learn has some convenience classes which will help fill in those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86065d30-fc00-4d16-a239-d74ddebb7d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9c1529-3c38-43b0-81f4-cf1ffdfd4453",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "trainDF_num = trainDF.drop('ocean_proximity', axis=1)\n",
    "\n",
    "median_imputer.fit(trainDF_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810743ea-6d85-4187-9b71-b8d62b25dd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = median_imputer.transform(trainDF_num)\n",
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bde010-e6ff-4add-9187-b08558733b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "trfDF_num = pd.DataFrame(\n",
    "    X,\n",
    "    columns=trainDF_num.columns, \n",
    "    index=trainDF_num.index\n",
    ")\n",
    "trfDF_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c105d82-ff1e-4733-9af4-e56bf50bcc1a",
   "metadata": {},
   "source": [
    "For the categorical values, we will use one hot encoding. You can do this explicitly with `pd.get_dummies`, but similar to what we did with the Imputer, using the corresponding sklearn class lets us reuse the transformation conveniently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ffbc0a-0ddc-48a3-b255-f457f915eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_encoder = OneHotEncoder()\n",
    "\n",
    "trainDF_cat = trainDF[['ocean_proximity']]\n",
    "X = cat_encoder.fit_transform(trainDF_cat)\n",
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18248b38-d723-41e4-a4cd-4d4a8eba5b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "trfDF_cat = pd.DataFrame(\n",
    "    X.toarray(),\n",
    "    columns = cat_encoder.categories_[0],\n",
    "    index = trainDF_cat.index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e6de43-9897-436e-803a-2d32fa15f142",
   "metadata": {},
   "source": [
    "#### Feature Scaling\n",
    "\n",
    "Generally, machine learning algorithms want to work with numerical values which share a similar scale. Taking logs etc. can help, but again sklearn also includes some convenience classes, we'll use `StandardScaler` which subtracts the mean and scales to unit variance. At this point, we'll also start building a transformation [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). Pipelines are important because your data tidyup and feature engineering is likely to get quite complicated, and your going to need to apply it many, many times while selecting and tuning your models. Packing the transformations up as a pipeline lets us do this reliably "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc98f43-015f-4e3b-befb-a155c427586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dd26d8-610d-49b9-a33f-c28a0f81cc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "X = num_pipeline.fit_transform(trainDF_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd52d8-4c0a-4ce4-b98d-3c7f0d78c005",
   "metadata": {},
   "outputs": [],
   "source": [
    "trfDF_num = pd.DataFrame(\n",
    "    X,\n",
    "    columns=trainDF_num.columns, \n",
    "    index=trainDF_num.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee080d3-05a6-44bd-924f-bff458788d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "trfDF_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39045247-b26e-4a7a-b6b5-2c2b60bee518",
   "metadata": {},
   "source": [
    "Now we can build a final pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7461d2c-50da-4896-9acb-914db0db20b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "trainDF = trainDForig.copy()\n",
    "trainDF.drop(trainDF[trainDF['median_house_value']>=500000].index, inplace=True)\n",
    "trainDF.reset_index(inplace=True, drop=True)\n",
    "\n",
    "housing_labels = trainDF['median_house_value']\n",
    "trainDF = trainDF.drop('median_house_value', axis='columns')\n",
    "\n",
    "rooms_ix      = housesDF.columns.get_loc('total_rooms')\n",
    "bedrooms_ix   = housesDF.columns.get_loc('total_bedrooms')\n",
    "population_ix = housesDF.columns.get_loc('population')\n",
    "households_ix = housesDF.columns.get_loc('households')\n",
    "\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "                         bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8909af-a81b-4555-bca0-1ac2c3e7917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('attr_addr',CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('num', num_pipeline, trainDF.select_dtypes('number').columns),\n",
    "    ('cat', OneHotEncoder(), trainDF.select_dtypes('object').columns),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9febbded-cd95-4ab7-b73b-ccfd377c1999",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDForig.copy()\n",
    "trainDF.drop(trainDF[trainDF['median_house_value']>=500000].index, inplace=True)\n",
    "trainDF.reset_index(inplace=True, drop=True)\n",
    "\n",
    "housing_labels = trainDF['median_house_value']\n",
    "trainDF = trainDF.drop('median_house_value', axis='columns')\n",
    "\n",
    "trf = full_pipeline.fit_transform(trainDF)\n",
    "\n",
    "tDF = pd.DataFrame(\n",
    "    full_pipeline.fit_transform(trainDF),\n",
    "    columns = (\n",
    "        list(trainDF.select_dtypes('number').columns) + \n",
    "        [\n",
    "            'rooms_per_household',\n",
    "            'population_per_household',\n",
    "            'bedrooms_per_room'\n",
    "        ] + \n",
    "        list(full_pipeline['cat'].get_feature_names_out())\n",
    "    ),\n",
    "    index = trainDF.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f589f9-f198-4121-90e2-31d5a7c6f3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24517666-cd61-4b51-a9f6-1c266bb4b94d",
   "metadata": {},
   "source": [
    "## Training a model\n",
    "\n",
    "After all of the hard work of preparing the data, the consistent design of scikit-learn lets us actually train a model very quickly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd559e3b-4e25-47e4-9b3e-0d4e441d2065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bea6c98-86e2-4bda-a97c-ba540b0d359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared = trf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a7c52-0166-4263-b151-75bade8aef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889b8226-9e6a-43d5-aa4e-b762a77173e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "housing_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f862e29-2ad1-4623-9cc8-93a195999150",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce5ac91-8e31-4892-8874-f64e017782fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfc572f-09a6-4a92-a3a3-f112f24ce1d5",
   "metadata": {},
   "source": [
    "Not great, not terrible. This is saying our predictions are off by around ~\\\\$60000 for this model. Ideally though, we want to get an idea of how reliable that figure is before proceeding. One way to do this is to hold out some more of our training data for this evaluation (remember we don't want to touch our actual test data until the very end). We could do this a bunch of times, using different segments of the training data and the look at how the root mean square error varies. That's the basic idea behind sklearn's K-fold cross validation feature. We will do a 10 fold cross validation for our model which will split the training data into 10 folds. The model will be trained 10 times in total using 9 of these folds as the training data and the remaining one to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703723cb-c8eb-4fa6-a69e-1375d35e651f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n",
    "                         scoring='neg_mean_squared_error', cv=10)\n",
    "lin_rmse_scores = np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4f45e0-f24f-4bf8-b35a-5eeea229b74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{lin_rmse_scores.mean()} += {lin_rmse_scores.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279b0ce4-1270-48a0-b533-b6662ad65742",
   "metadata": {},
   "source": [
    "At this point we could go back and try to improve the model by adding new features etc. but before we do that we should evaluate some other models to see how they do. Following [Aurélien's analysis](https://github.com/ageron/handson-ml2/blob/master/02_end_to_end_machine_learning_project.ipynb), we'll try a RandomForestRegressor model, notice how similar the steps are for the model, this consistent interface is the main benefit of scikit-learn and rewards all of the work put into the data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbe96d4-0fdb-4cb2-9170-ec3fbc22b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n",
    "                         scoring='neg_mean_squared_error', cv=3)\n",
    "forest_rmse_scores = np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39487024-9d04-41a1-8008-d1771d79822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{forest_rmse_scores.mean()} += {forest_rmse_scores.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f42181f-3543-4836-8866-a924f991b247",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e74a4ca-431d-4f86-ae3b-0427779e6dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    # try 12 (3×4) combinations of hyperparameters\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    # then try 6 (2×3) combinations with bootstrap set as False\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7365f9e6-40a5-4aad-aceb-cad08477aa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfd871e-acd3-4c8f-b2cb-71aed317db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32d3e6f-ba97-4b4c-bfb3-343e232e021e",
   "metadata": {},
   "source": [
    "We can look at the individual cross validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac79dac5-e005-4922-a234-81d43a4f434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b064f962-03b9-4c24-9833-2ade064e2e37",
   "metadata": {},
   "source": [
    "We have some surprisingly good results with e.g. 4 features and 10 estimators, but our best result is with `max_features=8, n_estimators=30`.\n",
    "\n",
    "One final thing we can do before we use our model is to look at how the various features fed into these results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc281ec-5012-4994-adfb-56a1883d4ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "\n",
    "extra_attribs = ['rooms_per_hhold', 'pop_per_hhold', 'bedrooms_per_room']\n",
    "cat_encoder = full_pipeline.named_transformers_['cat']\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "#attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "\n",
    "\n",
    "sorted(zip(feature_importances, tDF.columns), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb178cf-4705-4d16-999c-581c67b7a4be",
   "metadata": {},
   "source": [
    "Now we can try out our model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e224a788-46f0-4cf4-b315-89392deb8736",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test = testDF.drop(\"median_house_value\", axis=1)\n",
    "y_test = testDF[\"median_house_value\"].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "\n",
    "print(f\"{final_rmse}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8cd587e8-7149-4f94-a8d2-733c72c1bf18",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
